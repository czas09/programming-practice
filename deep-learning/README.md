#   Neural Networks

- - - - -

##  Activations

Common activation nonlinearities: 

Rectified linear units (ReLU)
Leaky rectified linear units
Exponential linear units (ELU)
Scaled exponential linear units
Softplus units
Hard sigmoid units
Exponential units
Hyperbolic tangent (tanh)
Logistic sigmoid
Affine

##  Losses

Common loss functions: 

Squared error
Categorical cross entropy
VAE Bernoulli loss
Wasserstein loss with gradient penalty
Noise contrastive estimation (NCE) loss

##  Wrappers

Layer wrappers: 

Dropout

##  Layers

Common layers / layer-wise operation that can be composed to create larger neural networks.

Fully-connected
Sparse evolutionary
Dot-product attention
1D and 2D convolution (with stride, padding, and dilation)
2D "deconvolution" (with stride and padding)
Restricted Boltzmann machines (with CD-_n_ training)
Elementwise multipulation
Embedding
Summation
Flattening
Softmax
Max & average pooling
1D and 2D batch normalization
1D and 2D layer normalization
Recurrent
Long short-term memory

##  Optimizers

Common modifications to stochastic gradient descent: 

SGD with momentum
AdaGrad
RMSProp
Adam

##  Learning Rate Schedulers

##  Initializers

##  Modules

##  Models

##  Utils
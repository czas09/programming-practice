#   PyTorch Documentation

- - - - -

##  1 `torch`

### 1.1 Tensors

1.1.1 Creation Ops
1.1.2 Indexing, Slicing, Joining, Mutating Ops

### 1.2 Generators

### 1.3 Random sampling

1.3.1 In-place random sampling
1.3.2 Quasi-random sampling

### 1.4 Serialization

### 1.5 Parallerism

### 1.6 Locally disabling gradient computation

### 1.7 Math operations

1.7.1 Pointwise Ops
1.7.2 Reduction Ops
1.7.3 Comparison Ops
1.7.4 Spectral Ops
1.7.5 Other Operations
1.7.6 BLAS and LAPACK Oprations

### 1.8 Utilities

- - - - -

##  2 `torch.nn`

### 2.1 Parameters

### 2.2 Containers

`Module`
`Sequential`
`ModuleList`
`ModuleDict`
`ParameterList`
`ParameterDict`

### 2.3 Convolution layers

`Conv1d`
`Conv2d`
`Conv3d`
`ConvTranspose1d`
`ConvTranspose2d`
`ConvTranspose3d`
`Unfold`
`Fold`

### 2.4 Pooling layers

`MaxPool1d`
`MaxPool2d`
`MaxPool3d`
`MaxUnool1d`
`MaxUnool2d`
`MaxUnool3d`
`AvgPool1d`
`AvgPool2d`
`AvgPool3d`
`FractionalMaxPool2d`
`LPPool1d`
`LPPool2d`
`AdaptiveMaxPool1d`
`AdaptiveMaxPool2d`
`AdaptiveMaxPool3d`
`AdaptiveAvgPool1d`
`AdaptiveAvgPool2d`
`AdaptiveAvgPool3d`

### 2.5 Padding layers

`ReflectionPad1d`
`ReflectionPad2d`
`ReplicationPad1d`
`ReplicationPad2d`
`ReplicationPad3d`
`ZeroPad2d`
`ConstantPad1d`
`ConstantPad2d`
`ConstantPad3d`

### 2.6 Non-linear activations (weighted sum, nonlinearity)

`ELU`
`Hardshrink`
`Hardtanh`
`LeakyReLU`
`LogSigmoid`
`MultiheadAttention`
`PReLU`
`ReLU`
`ReLU6`
`RReLU`
`SELU`
`CELU`
`GELU`
`Sigmoid`
`Softplus`
`Softshrink`
`Softsign`
`Tanh`
`Tanhshrink`
`Threshold`

### 2.7 Non-linear activations (other)

`Softmin`
`Softmax`
`Softmax2d`
`LogSoftmax`
`AdaptiveLogSoftmaxWithLoss`

### 2.8 Normalization layers

`BatchNorm1d`
`BatchNorm2d`
`BatchNorm3d`
`GroupNorm`
`SynBatchNorm`
`InstanceNorm1d`
`InstanceNorm2d`
`InstanceNorm3d`
`LayerNorm`
`LocalResponseNorm`

### 2.9 Recurrent layers

`RNNBase`
`RNN`
`LSTM`
`GRU`
`RNNCell`
`LSTMCell`
`GRUCell`

### 2.10 Transformer layers

`Transformer`
`TransformerEncoder`
`TransformerDecoder`
`TransformerEncoderLayer`
`TransformerDecoderLayer`

### 2.11 Linear layers

### 2.12 Dropout layers

### 2.13 Sparser layers

### 2.14 Distance functions

### 2.15 Loss functions

### 2.16 Vision layers

### 2.17 DataParrallel layers (multi-GPU, distributed)

### 2.18 Utilities

### 2.19 Quantized Functions

- - - - -

##  3 `torch.nn.functional`

##  4 `torch.Tensor`

##  5 tensor attributes

##  6 tensor views

##  7 `torch.autograd`

##  8 `torch.cuda`

##  9 `torch.cuda.amp`

##  10 `torch.distributed`

##  11 `torch.distributions`

##  12 `torch.hub`

##  13 `torch.jit`

##  14 `torch.nn.init`

##  15 `torch.onnx`

##  16 `torch.optim`

##  17 Quantization

##  18 Distributed RPC Framework

##  19 `torch.random`

##  20 `torch.sparse`

##  21 `torch.Storage`

##  22 `torch.utils.bottleneck`

##  23 `torch.utils.checkpoint`

##  24 `torch.utils.cpp_extension`

##  25 `torch.utils.data`

##  26 `torch.utils.dlpack`

##  27 `torch.utils.model_zoo`

##  28 `torch.utils.tensorboard`

##  29 Type Info

##  30 Named Tensors

##  31 Named Tensors operator coverage

##  32 `torch.__config__`

libraries:
    torchaudio
    torchtext
    torchvision
    TorchElastic
    TorchServe
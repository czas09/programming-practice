#   PyTorch Documentation

- - - - -

##  1 `torch`

### 1.1 Tensors

1.1.1 Creation Ops
1.1.2 Indexing, Slicing, Joining, Mutating Ops

### 1.2 Generators

### 1.3 Random sampling

1.3.1 In-place random sampling
1.3.2 Quasi-random sampling

### 1.4 Serialization

### 1.5 Parallerism

### 1.6 Locally disabling gradient computation

### 1.7 Math operations

1.7.1 Pointwise Ops
1.7.2 Reduction Ops
1.7.3 Comparison Ops
1.7.4 Spectral Ops
1.7.5 Other Operations
1.7.6 BLAS and LAPACK Oprations

### 1.8 Utilities

- - - - -

##  2 `torch.nn`

### 2.1 Parameters

### 2.2 Containers

2.2.1 Module
2.2.2 Sequential
2.2.3 ModuleList
2.2.4 ModuleDict
2.2.5 ParameterList
2.2.6 ParameterDict

### 2.3 Convolution layers

2.3.1 Conv1d
2.3.2 Conv2d
2.3.3 Conv3d
2.3.4 ConvTranspose1d
2.3.5 ConvTranspose2d
2.3.6 ConvTranspose3d
2.3.7 Unfold
2.3.8 Fold

### 2.4 Pooling layers

2.4.1 MaxPool1d
2.4.2 MaxPool2d
2.4.3 MaxPool3d
2.4.4 MaxUnool1d
2.4.5 MaxUnool2d
2.4.6 MaxUnool3d
2.4.7 AvgPool1d
2.4.8 AvgPool2d
2.4.9 AvgPool3d
2.4.10 FractionalMaxPool2d
2.4.11 LPPool1d
2.4.12 LPPool2d
2.4.13 AdaptiveMaxPool1d
2.4.14 AdaptiveMaxPool2d
2.4.15 AdaptiveMaxPool3d
2.4.16 AdaptiveAvgPool1d
2.4.17 AdaptiveAvgPool2d
2.4.18 AdaptiveAvgPool3d

### 2.5 Padding layers

2.5.1 ReflectionPad1d
2.5.2 ReflectionPad2d
2.5.3 ReplicationPad1d
2.5.4 ReplicationPad2d
2.5.5 ReplicationPad3d
2.5.6 ZeroPad2d
2.5.7 ConstantPad1d
2.5.8 ConstantPad2d
2.5.9 ConstantPad3d

### 2.6 Non-linear activations (weighted sum, nonlinearity)

2.6.1 ELU
2.6.2 Hardshrink
2.6.3 Hardtanh
2.6.4 LeakyReLU
2.6.5 LogSigmoid
2.6.6 MultiheadAttention
2.6.7 PReLU
2.6.8 ReLU
2.6.9 ReLU6
2.6.10 RReLU
2.6.11 SELU
2.6.12 CELU
2.6.13 GELU
2.6.14 Sigmoid
2.6.15 Softplus
2.6.16 Softshrink
2.6.17 Softsign
2.6.18 Tanh
2.6.19 Tanhshrink
2.6.20 Threshold

### 2.7 Non-linear activations (other)

2.7.1 Softmin
2.7.2 Softmax
2.7.3 Softmax2d
2.7.4 LogSoftmax
2.7.5 AdaptiveLogSoftmaxWithLoss

### 2.8 Normalization layers

2.8.1 BatchNorm1d
2.8.2 BatchNorm2d
2.8.3 BatchNorm3d
2.8.4 GroupNorm
2.8.5 SynBatchNorm
2.8.6 InstanceNorm1d
2.8.7 InstanceNorm2d
2.8.8 InstanceNorm3d
2.8.9 LayerNorm
2.8.10 LocalResponseNorm

### 2.9 Recurrent layers

2.9.1 RNNBase
2.9.2 RNN
2.9.3 LSTM
2.9.4 GRU
2.9.5 RNNCell
2.9.6 LSTMCell
2.9.7 GRUCell

### 2.10 Transformer layers

2.10.1 Transformer
2.10.2 TransformerEncoder
2.10.3 TransformerDecoder
2.10.4 TransformerEncoderLayer
2.10.5 TransformerDecoderLayer

### 2.11 Linear layers

### 2.12 Dropout layers

### 2.13 Sparser layers

### 2.14 Distance functions

### 2.15 Loss functions

### 2.16 Vision layers

### 2.17 DataParrallel layers (multi-GPU, distributed)

### 2.18 Utilities

### 2.19 Quantized Functions

- - - - -

##  3 `torch.nn.functional`

##  4 `torch.Tensor`

##  5 tensor attributes

##  6 tensor views

##  7 `torch.autograd`

##  8 `torch.cuda`

##  9 `torch.cuda.amp`

##  10 `torch.distributed`

##  11 `torch.distributions`

##  12 `torch.hub`

##  13 `torch.jit`

##  14 `torch.nn.init`

##  15 `torch.onnx`

##  16 `torch.optim`

##  17 Quantization

##  18 Distributed RPC Framework

##  19 `torch.random`

##  20 `torch.sparse`

##  21 `torch.Storage`

##  22 `torch.utils.bottleneck`

##  23 `torch.utils.checkpoint`

##  24 `torch.utils.cpp_extension`

##  25 `torch.utils.data`

##  26 `torch.utils.dlpack`

##  27 `torch.utils.model_zoo`

##  28 `torch.utils.tensorboard`

##  29 Type Info

##  30 Named Tensors

##  31 Named Tensors operator coverage

##  32 `torch.__config__`

libraries:
    torchaudio
    torchtext
    torchvision
    TorchElastic
    TorchServe